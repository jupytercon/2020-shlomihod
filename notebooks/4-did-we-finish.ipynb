{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop // Exploring Gender Bias in Word Embedding\n",
    "\n",
    "## https://learn.responsibly.ai/word-embedding\n",
    "\n",
    "Powerd by [`responsibly`](https://docs.responsibly.ai/) - Toolkit for auditing and mitigating bias and fairness of machine learning systems ğŸ”ğŸ¤–ğŸ§°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ’ Part Nine: Have we really removed the bias?\n",
    "\n",
    "Let's look on another metric, called **WEAT** (Word Embedding Association Test) which is inspired by **IAT** (Implicit-Association Test) from Pyschology.\n",
    "\n",
    "**Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). [Semantics derived automatically from language corpora contain human-like biases.](http://www.cs.bath.ac.uk/~jjb/ftp/CaliskanEtAl-authors-full.pdf) Science, 356(6334), 183-186.**\n",
    "\n",
    "\n",
    "## 9.1 - Ingredients\n",
    "\n",
    "1. Target words (e.g., Male ve. Female)\n",
    "\n",
    "2. Attribute words (e.g., Math vs. Arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy  # ğŸ› ï¸ For copying a nested data structure in Python\n",
    "\n",
    "from responsibly.we.weat import WEAT_DATA\n",
    "\n",
    "\n",
    "# B. A. Nosek, M. R. Banaji, A. G. Greenwald, Math=male, me=female, therefore mathâ‰ me.,\n",
    "# Journal of Personality and Social Psychology 83, 44 (2002).\n",
    "weat_gender_science_arts = deepcopy(WEAT_DATA[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ› ï¸ filter words from the original IAT experiment that are not presend in the reduced Word2Vec model\n",
    "\n",
    "from responsibly.we.weat import _filter_by_model_weat_stimuli\n",
    "\n",
    "_filter_by_model_weat_stimuli(weat_gender_science_arts, w2v_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['first_attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['second_attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['first_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_gender_science_arts['second_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 - Recipe\n",
    "\n",
    "â• Male x Science\n",
    "\n",
    "â– Male x Arts\n",
    "\n",
    "â– Female x Science\n",
    "\n",
    "â• Female x Arts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_combination_similiarity(model, attribute, target):\n",
    "    score = 0\n",
    "\n",
    "    for attribute_word in attribute['words']:\n",
    "\n",
    "        for target_word in target['words']:\n",
    "\n",
    "            score += w2v_small.similarity(attribute_word,\n",
    "                                          target_word)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_science_score = calc_combination_similiarity(w2v_small,\n",
    "                                                  weat_gender_science_arts['first_attribute'],\n",
    "                                                  weat_gender_science_arts['first_target'])\n",
    "\n",
    "male_science_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_arts_score = calc_combination_similiarity(w2v_small,\n",
    "                                               weat_gender_science_arts['first_attribute'],\n",
    "                                               weat_gender_science_arts['second_target'])\n",
    "\n",
    "male_arts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_science_score = calc_combination_similiarity(w2v_small,\n",
    "                                                    weat_gender_science_arts['second_attribute'],\n",
    "                                                    weat_gender_science_arts['first_target'])\n",
    "\n",
    "female_science_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_arts_score = calc_combination_similiarity(w2v_small,\n",
    "                                                 weat_gender_science_arts['second_attribute'],\n",
    "                                                 weat_gender_science_arts['second_target'])\n",
    "\n",
    "female_arts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_science_score - male_arts_score - female_science_score + female_arts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weat_gender_science_arts['first_attribute']['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(male_science_score - male_arts_score - female_science_score + female_arts_score) / 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 - All WEAT Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import calc_all_weat\n",
    "\n",
    "calc_all_weat(w2v_small, [weat_gender_science_arts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ Important Note: Our results are a bit different because we use a reduced Word2Vec.\n",
    "\n",
    "\n",
    "### Results from the Paper (computed on the complete Word2Vec):\n",
    "\n",
    "![](../images/weat-w2v.png)\n",
    "\n",
    "\n",
    "### âš¡Caveats\n",
    "\n",
    "#### Comparing WEAT to the IAT\n",
    "\n",
    "- Individuals (IAT) vs. Words (WEAT)\n",
    "- Therefore, the meanings of the effect size and p-value are totally different!\n",
    "\n",
    "#### âš¡ğŸ¦„ WEAT score definition\n",
    "\n",
    "The definition of the WEAT score is structured differently (but it is computationally equivalent). The original formulation matters to compute the p-value. Refer to the paper for details.\n",
    "\n",
    "### ğŸ§ª Effect size comparision between human and machine bias\n",
    "\n",
    "With the effect size, we can \"compare\" a human bias to a machine one. It raises the question whether the baseline for meauring bias/fairness of a machine should be human bias? Then a well-performing machine shouldn't be necessarily not biased, but only less biased than human (think about autonomous cars or semi-structured vs. unstructured interview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 - WEAT vs. IAT\n",
    "\n",
    "Lewis, M., & Lupyan, G. [What are we learning from language? Associations between gender biases and distributional semantics in 25 languages](https://mollylewis.shinyapps.io/iatlang_SI/).\n",
    "\n",
    "![](../images/iat-weat.png)\n",
    "\n",
    "\n",
    "1. Implicit male-career association (adjusted for participant age, gender, and congruent/incongruent block order) as a function of the linguistic male-career association derived from word-embeddings *r*(23) = 0.48 [0.11, 0.74]; *p* = 0.01; *n* = 25; Study 1b). Each point corresponds to a language. The size of the point is proportional to the number of participants who come from the country in which the language is dominant (total = 656,636 participants). Linguistic associations are estimated from models trained on text in each language from the Wikipedia corpus. Larger values indicate a greater tendency to associate men with the concept of career and women with the concept of family.\n",
    "\n",
    "2. Difference (UK minus US) in implicit association versus linguistic association for 31 IAT types (*N* = 27,045 participants). Error bands indicate standard error of the linear model estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 - Let's go back to our question - did we removed the bias?\n",
    "\n",
    "\n",
    "Gonen, H., & Goldberg, Y. (2019, June). [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://arxiv.org/pdf/1903.03862.pdf). In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 609-614).\n",
    "\n",
    "They used multiple methods, we'll show only two:\n",
    "1. WEAT\n",
    "2. Neutral words clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.1 -  WEAT - before and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/weat-experiment.png)\n",
    "\n",
    "See `responsibly` [demo page on word embedding](https://docs.responsibly.ail/notebooks/demo-word-embedding-bias.html#first-experiment-weat-before-and-after-debias) for a complete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.2 - Clustering Neutral Gender Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab = {word for word in w2v_small_gender_bias.model.vocab.keys()}\n",
    "\n",
    "# ğŸ¦„ how we got these words - read the Bolukbasi's paper for details\n",
    "all_gender_specific_words = set(BOLUKBASI_DATA['gender']['specific_full_with_definitional_equalize'])\n",
    "\n",
    "all_gender_neutral_words = w2v_vocab - all_gender_specific_words\n",
    "\n",
    "print('#vocab =', len(w2v_vocab),\n",
    "      '#specific =', len(all_gender_specific_words),\n",
    "      '#neutral =', len(all_gender_neutral_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections = [(w2v_small_gender_bias.project_on_direction(word), word)\n",
    "                                    for word in all_gender_neutral_words]\n",
    "\n",
    "neutral_words_gender_projections.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neutral_words_gender_projections[:-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words_gender_projections[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neutral words: top 500 male-biased and top 500 female-biased words\n",
    "\n",
    "GenderBiasWE.plot_most_biased_clustering(w2v_small_gender_bias, w2v_small_gender_debias);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In the paper, they got a stronger result, 92.5% accuracy for the debiased model.\n",
    "However, they perform clustering on all the words from the reduced word embedding, both gender- neutral and specific words, and applied slightly different pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.3 - ğŸ’ Strong words form the paper (emphasis mine):\n",
    "\n",
    "> The experiments ...\n",
    "reveal a **systematic bias** found in the embeddings,\n",
    "which is **independent of the gender direction**.\n",
    "\n",
    "\n",
    "> The implications are alarming: while suggested\n",
    "debiasing methods work well at removing the gender direction, the **debiasing is mostly superficial**.\n",
    "The bias stemming from world stereotypes and\n",
    "learned from the corpus is **ingrained much more\n",
    "deeply** in the embeddings space.\n",
    "\n",
    "\n",
    "> .. real concern from biased representations is **not the association** of a concept with\n",
    "words such as â€œheâ€, â€œsheâ€, â€œboyâ€, â€œgirlâ€ **nor** being\n",
    "able to perform **gender-stereotypical word analogies**... algorithmic discrimination is more likely to happen by associating one **implicitly gendered** term with\n",
    "other implicitly gendered terms, or picking up on\n",
    "**gender-specific regularities** in the corpus by learning to condition on gender-biased words, and generalizing to other gender-biased words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ’ğŸ’ Part Ten: Meta \"So What?\" - II\n",
    "\n",
    "## Can we debias at all a word embedding?\n",
    "\n",
    "## Under some downstream use-cases, maybe the bias in the word embedding is desirable?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
