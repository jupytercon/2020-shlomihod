{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop // Exploring Gender Bias in Word Embedding\n",
    "\n",
    "## https://learn.responsibly.ai/word-embedding\n",
    "\n",
    "Powerd by [`responsibly`](https://docs.responsibly.ai/) - Toolkit for auditing and mitigating bias and fairness of machine learning systems üîéü§ñüß∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Five: Gender Bias\n",
    "\n",
    "**‚ö° We use the word *bias* merely as a technical term, without jugement of \"good\" or \"bad\". Later on we will put the bias into *human contextes* to evaluate it.**\n",
    "\n",
    "Keep in mind, the data is from Google News, the writers are professional journalists.\n",
    "\n",
    "Bolukbasi Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. [Man is to computer programmer as woman is to homemaker? debiasing word embeddings](https://arxiv.org/abs/1607.06520). NIPS 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Gender appropriate he-she analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# she:sister :: he:?\n",
    "# sister - she + he = ?\n",
    "\n",
    "w2v_small.most_similar(positive=['sister', 'he'],\n",
    "                       negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "queen-king\n",
    "waitress-waiter\n",
    "sister-brother\n",
    "mother-father\n",
    "ovarian_cancer-prostate_cancer\n",
    "convent-monastery\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Gender stereotype he-she analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small.most_similar(positive=['nurse', 'he'],\n",
    "                       negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sewing-carpentry\n",
    "nurse-doctor\n",
    "blond-burly\n",
    "giggle-chuckle\n",
    "sassy-snappy\n",
    "volleyball-football\n",
    "register_nurse-physician\n",
    "interior_designer-architect\n",
    "feminism-conservatism\n",
    "vocalist-guitarist\n",
    "diva-superstar\n",
    "cupcakes-pizzas\n",
    "housewife-shopkeeper\n",
    "softball-baseball\n",
    "cosmetics-pharmaceuticals\n",
    "petite-lanky\n",
    "charming-affable\n",
    "hairdresser-barber\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodological Issue: The unrestricted version of analogy generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(w2v_small,\n",
    "             positive=['nurse', 'he'],\n",
    "             negative=['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö° Be Aware: According to a recent paper, it seems that the method of generating analogies enforce producing gender sterotype ones!\n",
    "\n",
    "Nissim, M., van Noord, R., van der Goot, R. (2019). [Fair is Better than Sensational: Man is to Doctor as Woman is to Doctor](https://arxiv.org/abs/1905.09866).\n",
    "\n",
    "... and a [Twitter thread](https://twitter.com/adamfungi/status/1133865428663635968) between the authors of the two papares.\n",
    "\n",
    "My takeaway (and as well as of other researchers): Analogies are not approriate method to observe bias in word embedding.\n",
    "\n",
    "üß™ What if our methodology introduce a bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - üíé What can we take from analogies? Gender Direction!\n",
    "\n",
    "# $\\overrightarrow{she} - \\overrightarrow{he}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction = w2v_small['she'] - w2v_small['he']\n",
    "\n",
    "gender_direction /= norm(gender_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['architect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['interior_designer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö°Interprete carefully: The word *architect* appears in more contexts with *he* than with *she*, and vice versa for *interior designer*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü¶Ñ In practice, we calculate the gender direction using multiple definitional pair of words for better estimation (words may have more than one meaning):\n",
    "\n",
    "- woman - man\n",
    "- girl - boy\n",
    "- she - he\n",
    "- mother - father\n",
    "- daughter - son\n",
    "- gal - guy\n",
    "- female - male\n",
    "- her - his\n",
    "- herself - himself\n",
    "- Mary - John"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 - üíª Try some words by yourself\n",
    "‚ö° Keep in mind: You are performing exploratory data analysis, and not evaluate systematically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction @ w2v_small['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 - üíé So What?\n",
    "\n",
    "Downstream Application - Putting a system into a human context\n",
    "\n",
    "### Toy Example - Search Engine Ranking\n",
    "\n",
    "- \"MIT computer science PhD student\"\n",
    "- \"doctoral candidate\" ~ \"PhD student\"\n",
    "- John:computer programmer :: Mary:homemaker\n",
    "\n",
    "### Universal Embeddings\n",
    "- Pre-trained on a large corpus\n",
    "- Plugged in downstream task models (sentimental analysis, classification, translation ‚Ä¶)\n",
    "- Improvement of performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 - Measuring Bias in Word Embedding\n",
    "\n",
    "# Think-Pair-Shar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "**Basic Ideas: Use neutral-gender words!**\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Neutral Professions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 - Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import GenderBiasWE\n",
    "\n",
    "w2v_small_gender_bias = GenderBiasWE(w2v_small, only_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.positive_end, w2v_small_gender_bias.negative_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender direction\n",
    "w2v_small_gender_bias.direction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we.data import BOLUKBASI_DATA\n",
    "\n",
    "neutral_profession_names = BOLUKBASI_DATA['gender']['neutral_profession_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_profession_names[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Why `actor` is in the neutral profession names list while `actress` is not there?\n",
    "1. Due to the statistical nature of the method that is used to find the gender- specific and natural word\n",
    "2. That might be because `actor` nowadays is much more gender-neutral, compared to waiter-waitress (see [Wikipedia - The term Actress](https://en.wikipedia.org/wiki/Actor#The_term_actress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neutral_profession_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same of using the @ operator on the bias direction\n",
    "\n",
    "w2v_small_gender_bias.project_on_direction(neutral_profession_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's visualize the projections of professions (neutral and specific by the orthography) on the gender direction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_bias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA: Demo - Visualizing gender bias with [Word Clouds](http://wordbias.umiacs.umd.edu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 - Are the projections of occupation words on the gender direction related to the real world?\n",
    "\n",
    "Let's take the percentage of female in various occupations from the Labor Force Statistics of 2017 Population Survey.\n",
    "\n",
    "Taken from: https://arxiv.org/abs/1804.06876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter  # üõ†Ô∏è For idiomatic sorting in Python\n",
    "\n",
    "from responsibly.we.data import OCCUPATION_FEMALE_PRECENTAGE\n",
    "\n",
    "sorted(OCCUPATION_FEMALE_PRECENTAGE.items(), key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 8))\n",
    "\n",
    "w2v_small_gender_bias.plot_factual_association(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also: Word embeddings quantify 100 years of gender stereotypes\n",
    "\n",
    "Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2018). [Word embeddings quantify 100 years of gender and ethnic stereotypes](https://www.pnas.org/content/pnas/115/16/E3635.full.pdf). Proceedings of the National Academy of Sciences, 115(16), E3635-E3644.\n",
    "\n",
    "![](../images/gender-bias-over-decades.png)\n",
    "\n",
    "<small>Data: Google Books/Corpus of Historical American English (COHA)</small>\n",
    "\n",
    "Word embedding is sometimes used to analyze a collection of text in **digital humanities** - putting a system into a human context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Quite strong and interesting observation! We used \"external\" data which wan't used directly to create the word embedding.\n",
    "\n",
    "It takes us to think about the *data generation process* - in both cases it is the \"world\", but it will be difficult to argue for causality only in one direction:\n",
    "1. Text in newspapers\n",
    "2. Employment by gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 - Direct Bias Measure\n",
    "\n",
    "1. Project each **neutral profession names** on the gender direction\n",
    "2. Calculate the absolute value of each projection\n",
    "3. Average it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using responsibly\n",
    "\n",
    "w2v_small_gender_bias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what responsibly does:\n",
    "\n",
    "neutral_profession_projections = [w2v_small[word] @ w2v_small_gender_bias.direction\n",
    "                                  for word in neutral_profession_names]\n",
    "\n",
    "abs_neutral_profession_projections = [abs(proj) for proj in neutral_profession_projections]\n",
    "\n",
    "sum(abs_neutral_profession_projections) / len(abs_neutral_profession_projections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ What are the assumptions of the direct bias measure? How the choice of neutral word effect on the definition of the bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 - [EXTRA] Indirect Bias Measure\n",
    "Similarity due to shared \"gender direction\" projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.generate_closest_words_indirect_bias('softball',\n",
    "                                                           'football')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Six: Mitigating Bias\n",
    "\n",
    "> We intentionally do not reference the resulting embeddings as \"debiased\" or free from all gender bias, and\n",
    "prefer the term \"mitigating bias\" rather that \"debiasing,\" to guard against the misconception that the resulting\n",
    "embeddings are entirely \"safe\" and need not be critically evaluated for bias in downstream tasks. <small>James-Sorenson, H., & Alvarez-Melis, D. (2019). [Probabilistic Bias Mitigation in Word Embeddings](https://arxiv.org/pdf/1910.14497.pdf). arXiv preprint arXiv:1910.14497.</small>\n",
    "\n",
    "\n",
    "## 6.1 - Neutralize\n",
    "\n",
    "In this case, we will remove the gender projection from all the words, except the neutral-gender ones, and then normalize.\n",
    "\n",
    "ü¶Ñ We need to \"learn\" what are the gender-specific words in the vocabulary for a seed set of gender-specific words (by semi-automatic use of [WordNet](https://en.wikipedia.org/wiki/WordNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='neutralize', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home:',\n",
    "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('man:',\n",
    "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('woman:',\n",
    "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 8))\n",
    "\n",
    "w2v_small_gender_debias.plot_factual_association(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 [EXTRA] Equalize\n",
    "\n",
    "- Do you see that `man` and `woman` have a different projection on the gender direction? \n",
    "\n",
    "- It might cause to different similarity (distance) to neutral words, such as to `kitchen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLUKBASI_DATA['gender']['equalize_pairs'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 - Hard Debias = Neutralize + Equalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='hard', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home:',\n",
    "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('man:',\n",
    "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('woman:',\n",
    "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
    "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.calc_direct_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 - Compare Preformances\n",
    "\n",
    "After debiasing, the performance of the word embedding, using standard benchmarks, get only slightly worse!\n",
    "\n",
    "**‚ö†Ô∏è It might take few minutes to run!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_bias.evaluate_word_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_small_gender_debias.evaluate_word_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíé Part Seven: So What?\n",
    "\n",
    "We removed the gender bias, **as we defined it**, in a word embedding - Is there any impact on a downstream application?\n",
    "\n",
    "## First example: coreference resolution\n",
    "\n",
    "Zhao, J., Wang, T., Yatskar, M., Ordonez, V., & Chang, K. W. (2018). [Gender bias in coreference resolution: Evaluation and debiasing methods](https://par.nsf.gov/servlets/purl/10084252). NAACL-HLT 2018.\n",
    "\n",
    "\n",
    "### WinoBias Dataset\n",
    "![](../images/coref-example.png)\n",
    "\n",
    "\n",
    "### Stereotypical Occupations (the source of `responsibly.we.data.OCCUPATION_FEMALE_PRECENTAGE`)\n",
    "![](../images/coref-occupations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on *UW End-to-end Neural Coreference Resolution System*\n",
    "\n",
    "#### No Intervention - Baseline\n",
    "\n",
    "| Word Embedding | OnoNotes | Type 1 - Pro-stereotypical | Type 1 - Anti-stereotypical |  Avg |  Diff |\n",
    "|:--------------:|:--------:|:--------------------------:|:---------------------------:|:----:|:-----:|\n",
    "|    Original    |   67.7   |            76.0            |             49.4            | 62.7 | 26.6* |\n",
    "\n",
    "#### Intervention: Named-entity anonymization\n",
    "\n",
    "| Word Embedding | OnoNotes | Type 1 - Pro-stereotypical | Type 1 - Anti-stereotypical |  Avg |  Diff |\n",
    "|:--------------:|:--------:|:--------------------------:|:---------------------------:|:----:|:-----:|\n",
    "|    Original    |   66.4   |            73.5            |             51.2            | 62.6 | 21.3* |\n",
    "|  Hard Debiased |   66.5   |            67.2            |             59.3            | 63.2 |  7.9* |\n",
    "\n",
    "#### Interventions: Named-entity anonymization + Gender swapping\n",
    "\n",
    "| Word Embedding | OnoNotes | Type 1 - Pro-stereotypical | Type 1 - Anti-stereotypical |  Avg |  Diff |\n",
    "|:--------------:|:--------:|:--------------------------:|:---------------------------:|:----:|:-----:|\n",
    "|    Original    |   66.2   |            65.1            |             59.2            | 62.2 |  5.9* |\n",
    "|  Hard Debiased |   66.3   |            63.9            |             62.8            | 63.4 |  1.1  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second example: another bias mitigation method\n",
    "\n",
    "Zhao, J., Zhou, Y., Li, Z., Wang, W., & Chang, K. W. (2018). [Learning gender-neutral word embeddings](https://arxiv.org/pdf/1809.01496.pdf). EMNLP 2018.\n",
    "\n",
    "The mitigation method is tailor-made for GloVe training process.\n",
    "\n",
    "![](../images/gn-glove-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíéüíé Part Eight: Meta \"So What?\" - I\n",
    "\n",
    "## How should we definition of \"bias\" in word embedding?\n",
    "\n",
    "### 1. Intrinsic (e.g., direct bias)\n",
    "\n",
    "### 2. External - Downstream application (e.g., coreference resolution, classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
