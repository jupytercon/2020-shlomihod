{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop // Exploring Gender Bias in Word Embedding\n",
        "\n",
        "## https://learn.responsibly.ai/word-embedding\n",
        "\nPowerd by [`responsibly`](https://docs.responsibly.ai/) - Toolkit for auditing and mitigating bias and fairness of machine learning systems üîéü§ñüß∞"
      ],
      "metadata": {
        "id": "ZqMESMMaTM59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part Eleven: Your Turn!\n",
        "<big>‚å®Ô∏è</big>\n",
        "\nNote: The first two tasks require a basic background in Python programming. For the last task, you need some experience with Machine Learning and Natural Langauge Processing (NLP) as well."
      ],
      "metadata": {
        "id": "EVMLVzKaTM5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from responsibly.we import load_w2v_small\n",
        "\nw2v_small = load_w2v_small()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "BgsASXY2TM5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Racial bias\n",
        "\nLet's explor racial bias usint Tolga's approche. Will use the [`responsibly.we.BiasWordEmbedding`](http://docs.responsibly.ai/word-embedding-bias.html#ethically.we.bias.BiasWordEmbedding) class. `GenderBiasWE` is a sub-class of `BiasWordEmbedding`."
      ],
      "metadata": {
        "id": "gPVjrwpwTM6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from responsibly.we import BiasWordEmbedding\n",
        "\nw2v_small_racial_bias = BiasWordEmbedding(w2v_small, only_lower=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8gB82y7hTM6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üíéüíéüíé Identify the racial direction using the `sum` method"
      ],
      "metadata": {
        "id": "V5Pa-_OOTM6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "white_common_names = ['Emily', 'Anne', 'Jill', 'Allison', 'Laurie', 'Sarah', 'Meredith', 'Carrie',\n",
        "                      'Kristen', 'Todd', 'Neil', 'Geoffrey', 'Brett', 'Brendan', 'Greg', 'Matthew',\n",
        "                      'Jay', 'Brad']\n",
        "\n",
        "black_common_names = ['Aisha', 'Keisha', 'Tamika', 'Lakisha', 'Tanisha', 'Latoya', 'Kenya', 'Latonya',\n",
        "                      'Ebony', 'Rasheed', 'Tremayne', 'Kareem', 'Darnell', 'Tyrone', 'Hakim', 'Jamal',\n",
        "                      'Leroy', 'Jermaine']\n",
        "\n",
        "w2v_small_racial_bias._identify_direction('Whites', 'Blacks',\n",
        "                                          definitional=(white_common_names, black_common_names),\n",
        "                                          method='sum')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "d8NjJ3fPTM6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the neutral profession names to measure the racial bias"
      ],
      "metadata": {
        "id": "BQzNVMgITM6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from responsibly.we.data import BOLUKBASI_DATA\n",
        "\nneutral_profession_names = BOLUKBASI_DATA['gender']['neutral_profession_names']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "6VsIaNeTTM6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neutral_profession_names[:10]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "iQMUSBpKTM6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "f, ax = plt.subplots(1, figsize=(10, 10))\n",
        "\nw2v_small_racial_bias.plot_projection_scores(neutral_profession_names, n_extreme=20, ax=ax);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "E3appKv5TM6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the direct bias measure"
      ],
      "metadata": {
        "id": "QvmZXAdKTM6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code Here..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "80S8S-3hTM6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep exploring the racial bias"
      ],
      "metadata": {
        "id": "MSFdc0NWTM6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code Here..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "GDjRDxz_TM6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 - Your WEAT test\n",
        "\nOpen the [word embedding demo page in `responsibly` documentation](http://docs.responsibly.ai/notebooks/demo-word-embedding-bias.html#it-is-possible-also-to-expirements-with-new-target-word-sets-as-in-this-example-citizen-immigrant), and look on the use of the function `calc_weat_pleasant_unpleasant_attribute`. What was the attempt in that experiment? What was the result? Can you come up with other experiments?"
      ],
      "metadata": {
        "id": "g9mOVcXWTM6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from responsibly.we import calc_weat_pleasant_unpleasant_attribute"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hvlhI95-TM6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code Here..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "VMmc9vNwTM6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 - Sentiment Analysis\n",
        "\n",
        "For this task, you will need to have some background with NLP, and in particular, training text classifier in Python.\n",
        "\n",
        "One way to examine bias in word embeddings is through downstream application. Here we will use sentiment analysis classifier of tweets; given a tween, the system would infer the the *valence*/*intensity* of the sentiment expressed in a tweet. The valence is expressed as a real number between 0 and 1, where 0 represent the negetive and and 1 is for the positive end.\n",
        "\n",
        "The system is going to be rather simple, and cosists of three components:\n",
        "\n",
        "1. Preprocessing (e.g., removing stopwords and punctuation, [tockenization](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation))\n",
        "2. Transforming the tokens of a tweet into a signle 300 dimensional vector.\n",
        "3. Applying logistic regression to predict the valence.\n",
        "\n",
        "Our goal it to asses whehter \n",
        "\n",
        "You are going to build two versions of that system: (1) with the original word2vec; and (2) \n",
        "\n",
        "Kiritchenko, S., & Mohammad, S. M. (2018). [Examining gender and race bias in two hundred sentiment analysis systems](https://arxiv.org/pdf/1805.04508.pdf). arXiv preprint arXiv:1805.04508.\n",
        "\n",
        "[Equity Evaluation Corpus (EEC)](http://saifmohammad.com/WebPages/Biases-SA.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "9Fud0_MaTM6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "First, let's load the datasets \"Affect in Tweets\" taken from [SemEval 2018](https://competitions.codalab.org/competitions/17751#learn_the_details-datasets) competition. We have training, development and test datasets. We will use only the first and the last, but feel free to use the development dataset to tune select models and hyperparameters with cross validation.\n",
        "\n",
        "We have three columns:\n",
        "\n",
        "1. `Tweet` - The tweet itself as string, the input.\n",
        "2. `Intensity Score` - The sentiment's intensity of the tweet in the range [0, 1].\n",
        "3. `Affect Dimension` - You can ignore it. It is `'valence'` for all of the datapoints.\n"
      ],
      "metadata": {
        "id": "iZY369yFKD91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./SemEval2018-Task1-all-data/English/V-reg/2018-Valence-reg-En-train.txt',\n",
        "                       sep='\\t', index_col=0)\n",
        "dev_df = pd.read_csv('./SemEval2018-Task1-all-data/English/V-reg/2018-Valence-reg-En-dev.txt',\n",
        "                       sep='\\t', index_col=0)\n",
        "test_df = pd.read_csv('./SemEval2018-Task1-all-data/English/V-reg/2018-Valence-reg-En-test-gold.txt',\n",
        "                       sep='\\t', index_col=0)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-w6q4eEPKCu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A few examples\n",
        "\ntrain_df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rrtR0AaLKC1f",
        "outputId": "fe474dfa-11f1-455e-ca14-fc82ce7a5983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all the labels from real numbers into boolean values,\n",
        "# setting the threshold at 0.5, and creating a new column named\n",
        "# `label`\n",
        "\n",
        "train_df['label'] = train_df['Intensity Score'] > 0.5\n",
        "dev_df['label'] = dev_df['Intensity Score'] > 0.5\n",
        "test_df['label'] = test_df['Intensity Score'] > 0.5"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vuw2NxOJKCyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's download the word2voc **complete** word embedding (not filtered only to lower cased words), and load it using `gensim`."
      ],
      "metadata": {
        "id": "F8ktamj9Lzvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "RJLEhGpaLyH2",
        "outputId": "65db04c6-7549-43b6-872e-4c6d3740b91c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load the word2vec\n",
        "w2v_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',\n",
        "                                              binary=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UjmXQTC9Lw3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector embedding for a word\n",
        "w2v_model['home']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "mv2O-OS6KCsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether there is an embedding for a word\n",
        "'bazinga' in w2v_model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "T-eIToHETM6o",
        "outputId": "c97ad702-3b32-4830-d1a9-f3928963e2d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing & feature extraction\n",
        "\n",
        "Before we transform a tweet into a vector of 300 dimention, it should be broken into tokens (\"words\") and be cleaned. You can do so with various Python pakcages for NLP, such as [NLTK](https://www.nltk.org/) and \n",
        "[spaCy](https://spacy.io/). Feel free to use them if you would like to! We will use the preprocessing functionality that comes with [`gensim`](https://radimrehurek.com/gensim/parsing/preprocessing.html)."
      ],
      "metadata": {
        "id": "S8LfyJnzNIy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.parsing.preprocessing import (preprocess_string,\n",
        "                                          strip_tags,\n",
        "                                          strip_punctuation,\n",
        "                                          strip_multiple_whitespaces,\n",
        "                                          strip_numeric,\n",
        "                                          remove_stopwords)\n",
        "\n",
        "# We pick a subset of the default filters,\n",
        "# in particular, we do not take\n",
        "# strip_short() and stem_text().\n",
        "FILTERS = [strip_punctuation,\n",
        "           strip_tags,\n",
        "           strip_multiple_whitespaces,\n",
        "           strip_numeric,\n",
        "           remove_stopwords]\n",
        "\n",
        "# \n",
        "preprocess_string('This is a \"short\" text!', FILTERS)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "SbQRZlFIshUb",
        "outputId": "6040ac20-1cf4-4ef2-b66a-05467f8d695b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After prerocessing all the tweets, we get tokens. We transform each token into a 300d vector using the word embedding, and then compute the *average* vector. It will have 300d as well. This vector seves as the features values for each tweet. \n",
        "\n",
        "Note for this two possible pitfalls:\n",
        "\n",
        "1. Make sure that the token exists int he word embedding.\n",
        "2. Sometimes, there are tweets without any token found in the word embedding. Discard these tweets from the data. Keep in mind that you should discard the labels as well.\n",
        "\nWrite the function `generate_text_features(text, w2v)` that gets a string `text` and a word embedding `w2v` and produce the features of this text according to the method described above."
      ],
      "metadata": {
        "id": "o_pi5WFOOcsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_features(text, w2v):\n",
        "    pass  # Your Code Here..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "mEn8ecmrsutk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use this function to produce the features for all the three datasets (training, validation, test)."
      ],
      "metadata": {
        "id": "JDSaVGfzP0Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code Here..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "sl0aYxLNQTs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a classifier\n",
        "\n",
        "The next step is strightforward, train logistic regression on the dataset. Report the accuracy for the training and the test dataset.\n",
        "\nWe recommend using [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
      ],
      "metadata": {
        "id": "MEM-riKyvdKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code Here..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9eMRIjj3REvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate gender bias in the downstream appliation"
      ],
      "metadata": {
        "id": "bdDLoHIBRV_B"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "5-exercises.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}